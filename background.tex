\section{Background and state of the art}\label{sect:background}

In \cite{riedl2013}, the authors point out several research areas and open questions spawned by the recent evolution of the video game industry. In particular, they raise two important questions whose answer form the main effort of this proposal: ``How can an AI agent interact with players over very long periods of time?'' and ``How can a Game AI system support deep gameplay?''.

An agent able to persist for months or years in a dynamic and interactive environment is usually referred to as a \emph{lifelong agent}. The main challenge for this type of agent is to adapting to continuously evolving environments (e.g., an online game world progressively modified by multiple players interactions). In \cite{thrun1995lifelong} the authors tackle the problem introducing \emph{Lifelong Machine Learning} (LFM) and underlining many sub-challenges for a lifelong agent, such as \emph{knowledge transfer}, \emph{representation of complex information} and \emph{continuous learning}.

In 2013, two important directions for LML have been proposed. In \cite{eaton2013ella} we can found a state-of-the-art pure-ML algorithm for LML agents. On the contrary, \cite{silver2013} shows a framework for LML which adds \emph{Knowledge Representation} approaches to classical Machine Learning techniques to model complex lifelong agents and facilitate knowledge sharing and querying between agents.

With regards to the video game environment, there are actually three main widely used action-driven decision making techniques. The first, proposed in 2005 for the game \emph{Halo 2}\footnote{\url{http://en.wikipedia.org/wiki/Halo_2}}, are the \emph{Behavior Trees} (BTs) \cite{Isla05BehaviorTrees}. This is a technique that combines practical features from techniques used in artificial intelligence for video-games such as \emph{Hierarchical Finite State Machines} (HFSMs)  and \emph{Decision Trees} and provide an expressive and easy to understand formalism for describing NPCs reactive behavior. The second is a pro-active technique proposed in 2005 in the game \emph{F.E.A.R.}\footnote{\url{http://en.wikipedia.org/wiki/F.E.A.R.}} called \emph{Goal-Oriented Action Planning} (GOAP) \cite{orkin06fear}. This is technique an optimized and simplified version of the academic \emph{STRIPS} planning applied in video-games settings. The main difference is that, while the STRIPS \cite{fikes1972strips} planning represents states using predicate logic literals, GOAP planners adopt more practical representations and data structures that are directly related to the programming environment such as key-value pairs stored in a dictionary or a fixed array.
Finally, the \emph{Hierarchical Task Network} (HTN) \cite{DBLP:conf/ijcai/Sacerdoti75} \cite{htn94complexity} is another recent planning technique widely used by the gaming industry. The main idea behind the HTN is to decompose complex tasks into smaller ones until to reach primitive actions which can be directly executed by the NPC following a similar representation using preconditions and effects.

One of the main challenge of this research is to verify the possibility to use one of these existing methods, in combination with lifelong agents techniques, in order to develop a new \emph{lifelong NPC}.


